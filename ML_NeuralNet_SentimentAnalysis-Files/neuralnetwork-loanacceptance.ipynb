{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The file \"loanacceptance.csv\" contains various attributes of 500 customers based on which loan has either been granted or denied. You have to create a system that automatically decides whether to grant a loan or not to grant a loan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data file through pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFileThroughPandas(filename):\n",
    "    # Reads the entire data file\n",
    "    data = pd.read_csv(filename)\n",
    "    \n",
    "    att = data[[\"Marital Status\",\"Kids\",\"Annual Household Salary\",\"Loan Amount\",\"Car owner\", \"Education Level\"]]\n",
    "    lab = data[\"Loan Granted\"]\n",
    "    \n",
    "    # Sometimes the dataset needs normalization when the variables are of very different orders of magnitude\n",
    "    # Normalization is not necessary for decision trees or random forests but is necessary for neural networks\n",
    "    # Standard deviation based normalization\n",
    "    att=(att-att.mean())/att.std()\n",
    "    # Zero-to-One normalization\n",
    "    # att=(att-att.min())/(att.max()-att.min())\n",
    "    \n",
    "    return(att,lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 6)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "(att,lab) = readFileThroughPandas(\"loanacceptance.csv\")\n",
    "\n",
    "# shape of the variables\n",
    "print(att.shape)\n",
    "print(lab.shape)\n",
    "\n",
    "# Use the first 400 rows for training and the remaining rows for testing\n",
    "\n",
    "# x_train = att.loc[0:400]\n",
    "# y_train = lab.loc[0:400]\n",
    "\n",
    "# x_test = att.loc[400:]\n",
    "# y_test = lab.loc[400:]\n",
    "\n",
    "# Alternatively use the following code to choose the rows randomly\n",
    "x_train, x_test, y_train, y_test = train_test_split(att, lab, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.67686483\n",
      "Iteration 2, loss = 0.67017572\n",
      "Iteration 3, loss = 0.66376551\n",
      "Iteration 4, loss = 0.65715698\n",
      "Iteration 5, loss = 0.65075843\n",
      "Iteration 6, loss = 0.64470306\n",
      "Iteration 7, loss = 0.63848580\n",
      "Iteration 8, loss = 0.63292044\n",
      "Iteration 9, loss = 0.62659563\n",
      "Iteration 10, loss = 0.62124878\n",
      "Iteration 11, loss = 0.61532907\n",
      "Iteration 12, loss = 0.60997892\n",
      "Iteration 13, loss = 0.60439547\n",
      "Iteration 14, loss = 0.59901127\n",
      "Iteration 15, loss = 0.59370818\n",
      "Iteration 16, loss = 0.58820882\n",
      "Iteration 17, loss = 0.58304423\n",
      "Iteration 18, loss = 0.57787214\n",
      "Iteration 19, loss = 0.57291067\n",
      "Iteration 20, loss = 0.56798631\n",
      "Iteration 21, loss = 0.56293716\n",
      "Iteration 22, loss = 0.55800722\n",
      "Iteration 23, loss = 0.55307845\n",
      "Iteration 24, loss = 0.54830119\n",
      "Iteration 25, loss = 0.54360757\n",
      "Iteration 26, loss = 0.53870627\n",
      "Iteration 27, loss = 0.53404123\n",
      "Iteration 28, loss = 0.52924818\n",
      "Iteration 29, loss = 0.52449921\n",
      "Iteration 30, loss = 0.51992248\n",
      "Iteration 31, loss = 0.51526461\n",
      "Iteration 32, loss = 0.51062363\n",
      "Iteration 33, loss = 0.50610439\n",
      "Iteration 34, loss = 0.50149956\n",
      "Iteration 35, loss = 0.49701471\n",
      "Iteration 36, loss = 0.49246977\n",
      "Iteration 37, loss = 0.48807153\n",
      "Iteration 38, loss = 0.48346547\n",
      "Iteration 39, loss = 0.47922150\n",
      "Iteration 40, loss = 0.47472314\n",
      "Iteration 41, loss = 0.47046490\n",
      "Iteration 42, loss = 0.46622033\n",
      "Iteration 43, loss = 0.46200221\n",
      "Iteration 44, loss = 0.45761078\n",
      "Iteration 45, loss = 0.45358093\n",
      "Iteration 46, loss = 0.44919550\n",
      "Iteration 47, loss = 0.44503136\n",
      "Iteration 48, loss = 0.44082358\n",
      "Iteration 49, loss = 0.43674799\n",
      "Iteration 50, loss = 0.43243316\n",
      "Iteration 51, loss = 0.42845522\n",
      "Iteration 52, loss = 0.42430023\n",
      "Iteration 53, loss = 0.42057447\n",
      "Iteration 54, loss = 0.41634021\n",
      "Iteration 55, loss = 0.41241276\n",
      "Iteration 56, loss = 0.40827641\n",
      "Iteration 57, loss = 0.40431066\n",
      "Iteration 58, loss = 0.40043926\n",
      "Iteration 59, loss = 0.39626830\n",
      "Iteration 60, loss = 0.39251901\n",
      "Iteration 61, loss = 0.38882357\n",
      "Iteration 62, loss = 0.38488591\n",
      "Iteration 63, loss = 0.38119450\n",
      "Iteration 64, loss = 0.37733742\n",
      "Iteration 65, loss = 0.37384929\n",
      "Iteration 66, loss = 0.37010912\n",
      "Iteration 67, loss = 0.36663169\n",
      "Iteration 68, loss = 0.36316242\n",
      "Iteration 69, loss = 0.35969678\n",
      "Iteration 70, loss = 0.35633021\n",
      "Iteration 71, loss = 0.35308886\n",
      "Iteration 72, loss = 0.34985560\n",
      "Iteration 73, loss = 0.34677967\n",
      "Iteration 74, loss = 0.34353896\n",
      "Iteration 75, loss = 0.34053513\n",
      "Iteration 76, loss = 0.33748076\n",
      "Iteration 77, loss = 0.33463958\n",
      "Iteration 78, loss = 0.33173350\n",
      "Iteration 79, loss = 0.32900591\n",
      "Iteration 80, loss = 0.32621152\n",
      "Iteration 81, loss = 0.32352393\n",
      "Iteration 82, loss = 0.32087572\n",
      "Iteration 83, loss = 0.31826304\n",
      "Iteration 84, loss = 0.31586619\n",
      "Iteration 85, loss = 0.31334134\n",
      "Iteration 86, loss = 0.31097539\n",
      "Iteration 87, loss = 0.30862011\n",
      "Iteration 88, loss = 0.30626248\n",
      "Iteration 89, loss = 0.30410562\n",
      "Iteration 90, loss = 0.30187002\n",
      "Iteration 91, loss = 0.29964089\n",
      "Iteration 92, loss = 0.29745725\n",
      "Iteration 93, loss = 0.29534443\n",
      "Iteration 94, loss = 0.29330906\n",
      "Iteration 95, loss = 0.29120312\n",
      "Iteration 96, loss = 0.28928181\n",
      "Iteration 97, loss = 0.28722669\n",
      "Iteration 98, loss = 0.28540020\n",
      "Iteration 99, loss = 0.28349474\n",
      "Iteration 100, loss = 0.28169891\n",
      "Iteration 101, loss = 0.27976516\n",
      "Iteration 102, loss = 0.27806353\n",
      "Iteration 103, loss = 0.27629110\n",
      "Iteration 104, loss = 0.27457511\n",
      "Iteration 105, loss = 0.27290849\n",
      "Iteration 106, loss = 0.27107029\n",
      "Iteration 107, loss = 0.26951404\n",
      "Iteration 108, loss = 0.26785227\n",
      "Iteration 109, loss = 0.26624486\n",
      "Iteration 110, loss = 0.26466288\n",
      "Iteration 111, loss = 0.26312847\n",
      "Iteration 112, loss = 0.26159716\n",
      "Iteration 113, loss = 0.26013712\n",
      "Iteration 114, loss = 0.25863553\n",
      "Iteration 115, loss = 0.25721046\n",
      "Iteration 116, loss = 0.25575211\n",
      "Iteration 117, loss = 0.25435253\n",
      "Iteration 118, loss = 0.25295412\n",
      "Iteration 119, loss = 0.25159263\n",
      "Iteration 120, loss = 0.25023572\n",
      "Iteration 121, loss = 0.24887372\n",
      "Iteration 122, loss = 0.24755841\n",
      "Iteration 123, loss = 0.24622125\n",
      "Iteration 124, loss = 0.24488012\n",
      "Iteration 125, loss = 0.24361638\n",
      "Iteration 126, loss = 0.24231080\n",
      "Iteration 127, loss = 0.24108384\n",
      "Iteration 128, loss = 0.23978334\n",
      "Iteration 129, loss = 0.23857975\n",
      "Iteration 130, loss = 0.23737615\n",
      "Iteration 131, loss = 0.23620607\n",
      "Iteration 132, loss = 0.23493807\n",
      "Iteration 133, loss = 0.23374370\n",
      "Iteration 134, loss = 0.23257954\n",
      "Iteration 135, loss = 0.23139988\n",
      "Iteration 136, loss = 0.23030192\n",
      "Iteration 137, loss = 0.22921327\n",
      "Iteration 138, loss = 0.22819136\n",
      "Iteration 139, loss = 0.22705543\n",
      "Iteration 140, loss = 0.22607675\n",
      "Iteration 141, loss = 0.22504366\n",
      "Iteration 142, loss = 0.22399312\n",
      "Iteration 143, loss = 0.22303095\n",
      "Iteration 144, loss = 0.22205257\n",
      "Iteration 145, loss = 0.22106728\n",
      "Iteration 146, loss = 0.22009573\n",
      "Iteration 147, loss = 0.21914567\n",
      "Iteration 148, loss = 0.21817037\n",
      "Iteration 149, loss = 0.21733664\n",
      "Iteration 150, loss = 0.21642666\n",
      "Iteration 151, loss = 0.21549091\n",
      "Iteration 152, loss = 0.21464888\n",
      "Iteration 153, loss = 0.21370968\n",
      "Iteration 154, loss = 0.21289009\n",
      "Iteration 155, loss = 0.21215564\n",
      "Iteration 156, loss = 0.21114673\n",
      "Iteration 157, loss = 0.21034463\n",
      "Iteration 158, loss = 0.20955912\n",
      "Iteration 159, loss = 0.20870661\n",
      "Iteration 160, loss = 0.20802647\n",
      "Iteration 161, loss = 0.20719741\n",
      "Iteration 162, loss = 0.20635357\n",
      "Iteration 163, loss = 0.20561760\n",
      "Iteration 164, loss = 0.20481423\n",
      "Iteration 165, loss = 0.20404322\n",
      "Iteration 166, loss = 0.20331760\n",
      "Iteration 167, loss = 0.20252375\n",
      "Iteration 168, loss = 0.20173188\n",
      "Iteration 169, loss = 0.20102795\n",
      "Iteration 170, loss = 0.20023289\n",
      "Iteration 171, loss = 0.19950043\n",
      "Iteration 172, loss = 0.19880485\n",
      "Iteration 173, loss = 0.19801210\n",
      "Iteration 174, loss = 0.19727370\n",
      "Iteration 175, loss = 0.19651512\n",
      "Iteration 176, loss = 0.19572108\n",
      "Iteration 177, loss = 0.19503292\n",
      "Iteration 178, loss = 0.19419665\n",
      "Iteration 179, loss = 0.19347389\n",
      "Iteration 180, loss = 0.19268665\n",
      "Iteration 181, loss = 0.19193533\n",
      "Iteration 182, loss = 0.19115784\n",
      "Iteration 183, loss = 0.19045471\n",
      "Iteration 184, loss = 0.18964921\n",
      "Iteration 185, loss = 0.18884506\n",
      "Iteration 186, loss = 0.18820026\n",
      "Iteration 187, loss = 0.18738015\n",
      "Iteration 188, loss = 0.18673368\n",
      "Iteration 189, loss = 0.18597622\n",
      "Iteration 190, loss = 0.18532959\n",
      "Iteration 191, loss = 0.18466084\n",
      "Iteration 192, loss = 0.18398765\n",
      "Iteration 193, loss = 0.18335630\n",
      "Iteration 194, loss = 0.18261752\n",
      "Iteration 195, loss = 0.18197795\n",
      "Iteration 196, loss = 0.18129402\n",
      "Iteration 197, loss = 0.18064900\n",
      "Iteration 198, loss = 0.18005057\n",
      "Iteration 199, loss = 0.17943474\n",
      "Iteration 200, loss = 0.17889227\n",
      "Iteration 201, loss = 0.17817577\n",
      "Iteration 202, loss = 0.17758152\n",
      "Iteration 203, loss = 0.17699574\n",
      "Iteration 204, loss = 0.17636784\n",
      "Iteration 205, loss = 0.17577670\n",
      "Iteration 206, loss = 0.17518783\n",
      "Iteration 207, loss = 0.17461869\n",
      "Iteration 208, loss = 0.17407325\n",
      "Iteration 209, loss = 0.17347226\n",
      "Iteration 210, loss = 0.17293590\n",
      "Iteration 211, loss = 0.17238628\n",
      "Iteration 212, loss = 0.17184199\n",
      "Iteration 213, loss = 0.17135062\n",
      "Iteration 214, loss = 0.17084513\n",
      "Iteration 215, loss = 0.17030859\n",
      "Iteration 216, loss = 0.16975502\n",
      "Iteration 217, loss = 0.16927472\n",
      "Iteration 218, loss = 0.16873289\n",
      "Iteration 219, loss = 0.16829312\n",
      "Iteration 220, loss = 0.16775120\n",
      "Iteration 221, loss = 0.16726301\n",
      "Iteration 222, loss = 0.16675211\n",
      "Iteration 223, loss = 0.16627836\n",
      "Iteration 224, loss = 0.16575305\n",
      "Iteration 225, loss = 0.16549440\n",
      "Iteration 226, loss = 0.16482380\n",
      "Iteration 227, loss = 0.16433298\n",
      "Iteration 228, loss = 0.16386183\n",
      "Iteration 229, loss = 0.16341672\n",
      "Iteration 230, loss = 0.16295571\n",
      "Iteration 231, loss = 0.16247965\n",
      "Iteration 232, loss = 0.16210200\n",
      "Iteration 233, loss = 0.16156857\n",
      "Iteration 234, loss = 0.16115060\n",
      "Iteration 235, loss = 0.16074020\n",
      "Iteration 236, loss = 0.16026025\n",
      "Iteration 237, loss = 0.15979824\n",
      "Iteration 238, loss = 0.15942945\n",
      "Iteration 239, loss = 0.15898044\n",
      "Iteration 240, loss = 0.15862939\n",
      "Iteration 241, loss = 0.15816193\n",
      "Iteration 242, loss = 0.15774954\n",
      "Iteration 243, loss = 0.15732580\n",
      "Iteration 244, loss = 0.15693974\n",
      "Iteration 245, loss = 0.15651480\n",
      "Iteration 246, loss = 0.15609333\n",
      "Iteration 247, loss = 0.15573461\n",
      "Iteration 248, loss = 0.15530931\n",
      "Iteration 249, loss = 0.15496718\n",
      "Iteration 250, loss = 0.15452294\n",
      "Iteration 251, loss = 0.15409761\n",
      "Iteration 252, loss = 0.15378651\n",
      "Iteration 253, loss = 0.15337818\n",
      "Iteration 254, loss = 0.15295000\n",
      "Iteration 255, loss = 0.15255817\n",
      "Iteration 256, loss = 0.15215524\n",
      "Iteration 257, loss = 0.15174191\n",
      "Iteration 258, loss = 0.15135519\n",
      "Iteration 259, loss = 0.15088704\n",
      "Iteration 260, loss = 0.15049680\n",
      "Iteration 261, loss = 0.15010357\n",
      "Iteration 262, loss = 0.14970298\n",
      "Iteration 263, loss = 0.14931003\n",
      "Iteration 264, loss = 0.14890575\n",
      "Iteration 265, loss = 0.14852713\n",
      "Iteration 266, loss = 0.14812268\n",
      "Iteration 267, loss = 0.14779018\n",
      "Iteration 268, loss = 0.14733368\n",
      "Iteration 269, loss = 0.14693978\n",
      "Iteration 270, loss = 0.14663230\n",
      "Iteration 271, loss = 0.14618960\n",
      "Iteration 272, loss = 0.14578676\n",
      "Iteration 273, loss = 0.14542056\n",
      "Iteration 274, loss = 0.14504900\n",
      "Iteration 275, loss = 0.14475124\n",
      "Iteration 276, loss = 0.14432735\n",
      "Iteration 277, loss = 0.14400117\n",
      "Iteration 278, loss = 0.14371900\n",
      "Iteration 279, loss = 0.14324560\n",
      "Iteration 280, loss = 0.14288120\n",
      "Iteration 281, loss = 0.14256472\n",
      "Iteration 282, loss = 0.14220777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 283, loss = 0.14178368\n",
      "Iteration 284, loss = 0.14141875\n",
      "Iteration 285, loss = 0.14109961\n",
      "Iteration 286, loss = 0.14071756\n",
      "Iteration 287, loss = 0.14038515\n",
      "Iteration 288, loss = 0.14000493\n",
      "Iteration 289, loss = 0.13969392\n",
      "Iteration 290, loss = 0.13932267\n",
      "Iteration 291, loss = 0.13902620\n",
      "Iteration 292, loss = 0.13870801\n",
      "Iteration 293, loss = 0.13828496\n",
      "Iteration 294, loss = 0.13797286\n",
      "Iteration 295, loss = 0.13766717\n",
      "Iteration 296, loss = 0.13735992\n",
      "Iteration 297, loss = 0.13694316\n",
      "Iteration 298, loss = 0.13661708\n",
      "Iteration 299, loss = 0.13630569\n",
      "Iteration 300, loss = 0.13597700\n",
      "Iteration 301, loss = 0.13564696\n",
      "Iteration 302, loss = 0.13532147\n",
      "Iteration 303, loss = 0.13496692\n",
      "Iteration 304, loss = 0.13466959\n",
      "Iteration 305, loss = 0.13435253\n",
      "Iteration 306, loss = 0.13403847\n",
      "Iteration 307, loss = 0.13371105\n",
      "Iteration 308, loss = 0.13339755\n",
      "Iteration 309, loss = 0.13312493\n",
      "Iteration 310, loss = 0.13273810\n",
      "Iteration 311, loss = 0.13243407\n",
      "Iteration 312, loss = 0.13216079\n",
      "Iteration 313, loss = 0.13181991\n",
      "Iteration 314, loss = 0.13150889\n",
      "Iteration 315, loss = 0.13120968\n",
      "Iteration 316, loss = 0.13090038\n",
      "Iteration 317, loss = 0.13057769\n",
      "Iteration 318, loss = 0.13029613\n",
      "Iteration 319, loss = 0.12995476\n",
      "Iteration 320, loss = 0.12965952\n",
      "Iteration 321, loss = 0.12933562\n",
      "Iteration 322, loss = 0.12903358\n",
      "Iteration 323, loss = 0.12873737\n",
      "Iteration 324, loss = 0.12846380\n",
      "Iteration 325, loss = 0.12817773\n",
      "Iteration 326, loss = 0.12783948\n",
      "Iteration 327, loss = 0.12761247\n",
      "Iteration 328, loss = 0.12724671\n",
      "Iteration 329, loss = 0.12705290\n",
      "Iteration 330, loss = 0.12669581\n",
      "Iteration 331, loss = 0.12641411\n",
      "Iteration 332, loss = 0.12610029\n",
      "Iteration 333, loss = 0.12587073\n",
      "Iteration 334, loss = 0.12547762\n",
      "Iteration 335, loss = 0.12522864\n",
      "Iteration 336, loss = 0.12491008\n",
      "Iteration 337, loss = 0.12458722\n",
      "Iteration 338, loss = 0.12430894\n",
      "Iteration 339, loss = 0.12398307\n",
      "Iteration 340, loss = 0.12368818\n",
      "Iteration 341, loss = 0.12341241\n",
      "Iteration 342, loss = 0.12314010\n",
      "Iteration 343, loss = 0.12288182\n",
      "Iteration 344, loss = 0.12264106\n",
      "Iteration 345, loss = 0.12229536\n",
      "Iteration 346, loss = 0.12210068\n",
      "Iteration 347, loss = 0.12177193\n",
      "Iteration 348, loss = 0.12151574\n",
      "Iteration 349, loss = 0.12123417\n",
      "Iteration 350, loss = 0.12096577\n",
      "Iteration 351, loss = 0.12071294\n",
      "Iteration 352, loss = 0.12046472\n",
      "Iteration 353, loss = 0.12019107\n",
      "Iteration 354, loss = 0.11992264\n",
      "Iteration 355, loss = 0.11971007\n",
      "Iteration 356, loss = 0.11942718\n",
      "Iteration 357, loss = 0.11921230\n",
      "Iteration 358, loss = 0.11890484\n",
      "Iteration 359, loss = 0.11867329\n",
      "Iteration 360, loss = 0.11838867\n",
      "Iteration 361, loss = 0.11816981\n",
      "Iteration 362, loss = 0.11789813\n",
      "Iteration 363, loss = 0.11764716\n",
      "Iteration 364, loss = 0.11739880\n",
      "Iteration 365, loss = 0.11719340\n",
      "Iteration 366, loss = 0.11700255\n",
      "Iteration 367, loss = 0.11668377\n",
      "Iteration 368, loss = 0.11643009\n",
      "Iteration 369, loss = 0.11613835\n",
      "Iteration 370, loss = 0.11587013\n",
      "Iteration 371, loss = 0.11559306\n",
      "Iteration 372, loss = 0.11532841\n",
      "Iteration 373, loss = 0.11507820\n",
      "Iteration 374, loss = 0.11470522\n",
      "Iteration 375, loss = 0.11455123\n",
      "Iteration 376, loss = 0.11412764\n",
      "Iteration 377, loss = 0.11388434\n",
      "Iteration 378, loss = 0.11359059\n",
      "Iteration 379, loss = 0.11325694\n",
      "Iteration 380, loss = 0.11307215\n",
      "Iteration 381, loss = 0.11269446\n",
      "Iteration 382, loss = 0.11237776\n",
      "Iteration 383, loss = 0.11209638\n",
      "Iteration 384, loss = 0.11184776\n",
      "Iteration 385, loss = 0.11162401\n",
      "Iteration 386, loss = 0.11122399\n",
      "Iteration 387, loss = 0.11100331\n",
      "Iteration 388, loss = 0.11061771\n",
      "Iteration 389, loss = 0.11046590\n",
      "Iteration 390, loss = 0.11006271\n",
      "Iteration 391, loss = 0.10978684\n",
      "Iteration 392, loss = 0.10954518\n",
      "Iteration 393, loss = 0.10920306\n",
      "Iteration 394, loss = 0.10894724\n",
      "Iteration 395, loss = 0.10866332\n",
      "Iteration 396, loss = 0.10834938\n",
      "Iteration 397, loss = 0.10802399\n",
      "Iteration 398, loss = 0.10775289\n",
      "Iteration 399, loss = 0.10743072\n",
      "Iteration 400, loss = 0.10714525\n",
      "Iteration 401, loss = 0.10684589\n",
      "Iteration 402, loss = 0.10654808\n",
      "Iteration 403, loss = 0.10627110\n",
      "Iteration 404, loss = 0.10599031\n",
      "Iteration 405, loss = 0.10567794\n",
      "Iteration 406, loss = 0.10537384\n",
      "Iteration 407, loss = 0.10508167\n",
      "Iteration 408, loss = 0.10484726\n",
      "Iteration 409, loss = 0.10455653\n",
      "Iteration 410, loss = 0.10436186\n",
      "Iteration 411, loss = 0.10398306\n",
      "Iteration 412, loss = 0.10373907\n",
      "Iteration 413, loss = 0.10339444\n",
      "Iteration 414, loss = 0.10309378\n",
      "Iteration 415, loss = 0.10281761\n",
      "Iteration 416, loss = 0.10245359\n",
      "Iteration 417, loss = 0.10219321\n",
      "Iteration 418, loss = 0.10175081\n",
      "Iteration 419, loss = 0.10154343\n",
      "Iteration 420, loss = 0.10114821\n",
      "Iteration 421, loss = 0.10080246\n",
      "Iteration 422, loss = 0.10041082\n",
      "Iteration 423, loss = 0.10015265\n",
      "Iteration 424, loss = 0.09982939\n",
      "Iteration 425, loss = 0.09954038\n",
      "Iteration 426, loss = 0.09926050\n",
      "Iteration 427, loss = 0.09904570\n",
      "Iteration 428, loss = 0.09881396\n",
      "Iteration 429, loss = 0.09845640\n",
      "Iteration 430, loss = 0.09824605\n",
      "Iteration 431, loss = 0.09795032\n",
      "Iteration 432, loss = 0.09765057\n",
      "Iteration 433, loss = 0.09740740\n",
      "Iteration 434, loss = 0.09714589\n",
      "Iteration 435, loss = 0.09688215\n",
      "Iteration 436, loss = 0.09663444\n",
      "Iteration 437, loss = 0.09632600\n",
      "Iteration 438, loss = 0.09619891\n",
      "Iteration 439, loss = 0.09581777\n",
      "Iteration 440, loss = 0.09560773\n",
      "Iteration 441, loss = 0.09549994\n",
      "Iteration 442, loss = 0.09508841\n",
      "Iteration 443, loss = 0.09488882\n",
      "Iteration 444, loss = 0.09458109\n",
      "Iteration 445, loss = 0.09433362\n",
      "Iteration 446, loss = 0.09410189\n",
      "Iteration 447, loss = 0.09381869\n",
      "Iteration 448, loss = 0.09358845\n",
      "Iteration 449, loss = 0.09335334\n",
      "Iteration 450, loss = 0.09308521\n",
      "Iteration 451, loss = 0.09284915\n",
      "Iteration 452, loss = 0.09268136\n",
      "Iteration 453, loss = 0.09246368\n",
      "Iteration 454, loss = 0.09222945\n",
      "Iteration 455, loss = 0.09198876\n",
      "Iteration 456, loss = 0.09176379\n",
      "Iteration 457, loss = 0.09155774\n",
      "Iteration 458, loss = 0.09139742\n",
      "Iteration 459, loss = 0.09113300\n",
      "Iteration 460, loss = 0.09092459\n",
      "Iteration 461, loss = 0.09067750\n",
      "Iteration 462, loss = 0.09055259\n",
      "Iteration 463, loss = 0.09029907\n",
      "Iteration 464, loss = 0.09010108\n",
      "Iteration 465, loss = 0.08986436\n",
      "Iteration 466, loss = 0.08966900\n",
      "Iteration 467, loss = 0.08963627\n",
      "Iteration 468, loss = 0.08926937\n",
      "Iteration 469, loss = 0.08912113\n",
      "Iteration 470, loss = 0.08896409\n",
      "Iteration 471, loss = 0.08875998\n",
      "Iteration 472, loss = 0.08858181\n",
      "Iteration 473, loss = 0.08825154\n",
      "Iteration 474, loss = 0.08804663\n",
      "Iteration 475, loss = 0.08783395\n",
      "Iteration 476, loss = 0.08768090\n",
      "Iteration 477, loss = 0.08746978\n",
      "Iteration 478, loss = 0.08723322\n",
      "Iteration 479, loss = 0.08705034\n",
      "Iteration 480, loss = 0.08684558\n",
      "Iteration 481, loss = 0.08672465\n",
      "Iteration 482, loss = 0.08646322\n",
      "Iteration 483, loss = 0.08630406\n",
      "Iteration 484, loss = 0.08607657\n",
      "Iteration 485, loss = 0.08586246\n",
      "Iteration 486, loss = 0.08563895\n",
      "Iteration 487, loss = 0.08546258\n",
      "Iteration 488, loss = 0.08527125\n",
      "Iteration 489, loss = 0.08512580\n",
      "Iteration 490, loss = 0.08491143\n",
      "Iteration 491, loss = 0.08462119\n",
      "Iteration 492, loss = 0.08445296\n",
      "Iteration 493, loss = 0.08419579\n",
      "Iteration 494, loss = 0.08399143\n",
      "Iteration 495, loss = 0.08374244\n",
      "Iteration 496, loss = 0.08355185\n",
      "Iteration 497, loss = 0.08329565\n",
      "Iteration 498, loss = 0.08305813\n",
      "Iteration 499, loss = 0.08308853\n",
      "Iteration 500, loss = 0.08269741\n",
      "Iteration 501, loss = 0.08244165\n",
      "Iteration 502, loss = 0.08225344\n",
      "Iteration 503, loss = 0.08199169\n",
      "Iteration 504, loss = 0.08179566\n",
      "Iteration 505, loss = 0.08157122\n",
      "Iteration 506, loss = 0.08139890\n",
      "Iteration 507, loss = 0.08109515\n",
      "Iteration 508, loss = 0.08090653\n",
      "Iteration 509, loss = 0.08072326\n",
      "Iteration 510, loss = 0.08045705\n",
      "Iteration 511, loss = 0.08024665\n",
      "Iteration 512, loss = 0.08002439\n",
      "Iteration 513, loss = 0.07981876\n",
      "Iteration 514, loss = 0.07964023\n",
      "Iteration 515, loss = 0.07940791\n",
      "Iteration 516, loss = 0.07923780\n",
      "Iteration 517, loss = 0.07901658\n",
      "Iteration 518, loss = 0.07880043\n",
      "Iteration 519, loss = 0.07864524\n",
      "Iteration 520, loss = 0.07838364\n",
      "Iteration 521, loss = 0.07820788\n",
      "Iteration 522, loss = 0.07796880\n",
      "Iteration 523, loss = 0.07779121\n",
      "Iteration 524, loss = 0.07753755\n",
      "Iteration 525, loss = 0.07728332\n",
      "Iteration 526, loss = 0.07705244\n",
      "Iteration 527, loss = 0.07683090\n",
      "Iteration 528, loss = 0.07662661\n",
      "Iteration 529, loss = 0.07642134\n",
      "Iteration 530, loss = 0.07618680\n",
      "Iteration 531, loss = 0.07598991\n",
      "Iteration 532, loss = 0.07578731\n",
      "Iteration 533, loss = 0.07554807\n",
      "Iteration 534, loss = 0.07536823\n",
      "Iteration 535, loss = 0.07514916\n",
      "Iteration 536, loss = 0.07496578\n",
      "Iteration 537, loss = 0.07479312\n",
      "Iteration 538, loss = 0.07457807\n",
      "Iteration 539, loss = 0.07438846\n",
      "Iteration 540, loss = 0.07425385\n",
      "Iteration 541, loss = 0.07400038\n",
      "Iteration 542, loss = 0.07382120\n",
      "Iteration 543, loss = 0.07368708\n",
      "Iteration 544, loss = 0.07347467\n",
      "Iteration 545, loss = 0.07332951\n",
      "Iteration 546, loss = 0.07314885\n",
      "Iteration 547, loss = 0.07295284\n",
      "Iteration 548, loss = 0.07280277\n",
      "Iteration 549, loss = 0.07260431\n",
      "Iteration 550, loss = 0.07247946\n",
      "Iteration 551, loss = 0.07228990\n",
      "Iteration 552, loss = 0.07209442\n",
      "Iteration 553, loss = 0.07193448\n",
      "Iteration 554, loss = 0.07175610\n",
      "Iteration 555, loss = 0.07157945\n",
      "Iteration 556, loss = 0.07136993\n",
      "Iteration 557, loss = 0.07125510\n",
      "Iteration 558, loss = 0.07104559\n",
      "Iteration 559, loss = 0.07092306\n",
      "Iteration 560, loss = 0.07078051\n",
      "Iteration 561, loss = 0.07051242\n",
      "Iteration 562, loss = 0.07040723\n",
      "Iteration 563, loss = 0.07023407\n",
      "Iteration 564, loss = 0.07005776\n",
      "Iteration 565, loss = 0.06990256\n",
      "Iteration 566, loss = 0.06971414\n",
      "Iteration 567, loss = 0.06954585\n",
      "Iteration 568, loss = 0.06947547\n",
      "Iteration 569, loss = 0.06918205\n",
      "Iteration 570, loss = 0.06900268\n",
      "Iteration 571, loss = 0.06887928\n",
      "Iteration 572, loss = 0.06866791\n",
      "Iteration 573, loss = 0.06850229\n",
      "Iteration 574, loss = 0.06829388\n",
      "Iteration 575, loss = 0.06818398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 576, loss = 0.06803405\n",
      "Iteration 577, loss = 0.06782079\n",
      "Iteration 578, loss = 0.06767935\n",
      "Iteration 579, loss = 0.06758470\n",
      "Iteration 580, loss = 0.06733505\n",
      "Iteration 581, loss = 0.06718662\n",
      "Iteration 582, loss = 0.06702232\n",
      "Iteration 583, loss = 0.06693293\n",
      "Iteration 584, loss = 0.06676675\n",
      "Iteration 585, loss = 0.06673082\n",
      "Iteration 586, loss = 0.06644249\n",
      "Iteration 587, loss = 0.06632488\n",
      "Iteration 588, loss = 0.06611495\n",
      "Iteration 589, loss = 0.06604948\n",
      "Iteration 590, loss = 0.06590174\n",
      "Iteration 591, loss = 0.06573245\n",
      "Iteration 592, loss = 0.06553238\n",
      "Iteration 593, loss = 0.06546877\n",
      "Iteration 594, loss = 0.06529850\n",
      "Iteration 595, loss = 0.06528936\n",
      "Iteration 596, loss = 0.06500142\n",
      "Iteration 597, loss = 0.06488861\n",
      "Iteration 598, loss = 0.06468585\n",
      "Iteration 599, loss = 0.06463285\n",
      "Iteration 600, loss = 0.06442993\n",
      "Iteration 601, loss = 0.06423259\n",
      "Iteration 602, loss = 0.06414586\n",
      "Iteration 603, loss = 0.06393090\n",
      "Iteration 604, loss = 0.06387131\n",
      "Iteration 605, loss = 0.06367643\n",
      "Iteration 606, loss = 0.06350959\n",
      "Iteration 607, loss = 0.06333094\n",
      "Iteration 608, loss = 0.06318260\n",
      "Iteration 609, loss = 0.06307283\n",
      "Iteration 610, loss = 0.06293010\n",
      "Iteration 611, loss = 0.06273464\n",
      "Iteration 612, loss = 0.06260276\n",
      "Iteration 613, loss = 0.06242609\n",
      "Iteration 614, loss = 0.06226939\n",
      "Iteration 615, loss = 0.06201706\n",
      "Iteration 616, loss = 0.06186011\n",
      "Iteration 617, loss = 0.06182382\n",
      "Iteration 618, loss = 0.06153177\n",
      "Iteration 619, loss = 0.06134045\n",
      "Iteration 620, loss = 0.06118311\n",
      "Iteration 621, loss = 0.06107920\n",
      "Iteration 622, loss = 0.06092621\n",
      "Iteration 623, loss = 0.06071133\n",
      "Iteration 624, loss = 0.06059275\n",
      "Iteration 625, loss = 0.06040722\n",
      "Iteration 626, loss = 0.06028701\n",
      "Iteration 627, loss = 0.06016161\n",
      "Iteration 628, loss = 0.05998976\n",
      "Iteration 629, loss = 0.05996691\n",
      "Iteration 630, loss = 0.05975230\n",
      "Iteration 631, loss = 0.05949732\n",
      "Iteration 632, loss = 0.05935514\n",
      "Iteration 633, loss = 0.05918754\n",
      "Iteration 634, loss = 0.05902387\n",
      "Iteration 635, loss = 0.05897768\n",
      "Iteration 636, loss = 0.05872070\n",
      "Iteration 637, loss = 0.05852692\n",
      "Iteration 638, loss = 0.05837644\n",
      "Iteration 639, loss = 0.05820236\n",
      "Iteration 640, loss = 0.05796365\n",
      "Iteration 641, loss = 0.05775691\n",
      "Iteration 642, loss = 0.05759604\n",
      "Iteration 643, loss = 0.05745874\n",
      "Iteration 644, loss = 0.05740772\n",
      "Iteration 645, loss = 0.05700848\n",
      "Iteration 646, loss = 0.05689688\n",
      "Iteration 647, loss = 0.05668351\n",
      "Iteration 648, loss = 0.05652130\n",
      "Iteration 649, loss = 0.05635073\n",
      "Iteration 650, loss = 0.05621688\n",
      "Iteration 651, loss = 0.05617746\n",
      "Iteration 652, loss = 0.05586795\n",
      "Iteration 653, loss = 0.05584568\n",
      "Iteration 654, loss = 0.05575427\n",
      "Iteration 655, loss = 0.05552586\n",
      "Iteration 656, loss = 0.05538950\n",
      "Iteration 657, loss = 0.05534754\n",
      "Iteration 658, loss = 0.05516993\n",
      "Iteration 659, loss = 0.05507784\n",
      "Iteration 660, loss = 0.05486462\n",
      "Iteration 661, loss = 0.05477342\n",
      "Iteration 662, loss = 0.05477666\n",
      "Iteration 663, loss = 0.05459521\n",
      "Iteration 664, loss = 0.05441067\n",
      "Iteration 665, loss = 0.05429889\n",
      "Iteration 666, loss = 0.05421779\n",
      "Iteration 667, loss = 0.05419327\n",
      "Iteration 668, loss = 0.05406040\n",
      "Iteration 669, loss = 0.05389853\n",
      "Iteration 670, loss = 0.05374831\n",
      "Iteration 671, loss = 0.05379261\n",
      "Iteration 672, loss = 0.05356551\n",
      "Iteration 673, loss = 0.05347110\n",
      "Iteration 674, loss = 0.05334981\n",
      "Iteration 675, loss = 0.05320003\n",
      "Iteration 676, loss = 0.05316821\n",
      "Iteration 677, loss = 0.05296360\n",
      "Iteration 678, loss = 0.05299473\n",
      "Iteration 679, loss = 0.05280847\n",
      "Iteration 680, loss = 0.05266047\n",
      "Iteration 681, loss = 0.05258271\n",
      "Iteration 682, loss = 0.05254793\n",
      "Iteration 683, loss = 0.05239086\n",
      "Iteration 684, loss = 0.05228745\n",
      "Iteration 685, loss = 0.05219201\n",
      "Iteration 686, loss = 0.05206169\n",
      "Iteration 687, loss = 0.05196797\n",
      "Iteration 688, loss = 0.05189889\n",
      "Iteration 689, loss = 0.05179489\n",
      "Iteration 690, loss = 0.05165032\n",
      "Iteration 691, loss = 0.05158410\n",
      "Iteration 692, loss = 0.05144137\n",
      "Iteration 693, loss = 0.05132156\n",
      "Iteration 694, loss = 0.05129291\n",
      "Iteration 695, loss = 0.05118018\n",
      "Iteration 696, loss = 0.05114618\n",
      "Iteration 697, loss = 0.05105316\n",
      "Iteration 698, loss = 0.05093024\n",
      "Iteration 699, loss = 0.05084898\n",
      "Iteration 700, loss = 0.05075276\n",
      "Iteration 701, loss = 0.05065852\n",
      "Iteration 702, loss = 0.05048409\n",
      "Iteration 703, loss = 0.05042157\n",
      "Iteration 704, loss = 0.05037017\n",
      "Iteration 705, loss = 0.05030774\n",
      "Iteration 706, loss = 0.05012338\n",
      "Iteration 707, loss = 0.05023949\n",
      "Iteration 708, loss = 0.04998511\n",
      "Iteration 709, loss = 0.04984311\n",
      "Iteration 710, loss = 0.04978031\n",
      "Iteration 711, loss = 0.04972220\n",
      "Iteration 712, loss = 0.04959833\n",
      "Iteration 713, loss = 0.04950484\n",
      "Iteration 714, loss = 0.04948805\n",
      "Iteration 715, loss = 0.04941447\n",
      "Iteration 716, loss = 0.04923356\n",
      "Iteration 717, loss = 0.04936995\n",
      "Iteration 718, loss = 0.04931437\n",
      "Iteration 719, loss = 0.04901064\n",
      "Iteration 720, loss = 0.04891733\n",
      "Iteration 721, loss = 0.04889684\n",
      "Iteration 722, loss = 0.04877600\n",
      "Iteration 723, loss = 0.04868801\n",
      "Iteration 724, loss = 0.04855259\n",
      "Iteration 725, loss = 0.04878220\n",
      "Iteration 726, loss = 0.04843130\n",
      "Iteration 727, loss = 0.04835170\n",
      "Iteration 728, loss = 0.04826543\n",
      "Iteration 729, loss = 0.04823889\n",
      "Iteration 730, loss = 0.04811470\n",
      "Iteration 731, loss = 0.04809751\n",
      "Iteration 732, loss = 0.04793487\n",
      "Iteration 733, loss = 0.04784421\n",
      "Iteration 734, loss = 0.04776095\n",
      "Iteration 735, loss = 0.04763857\n",
      "Iteration 736, loss = 0.04755911\n",
      "Iteration 737, loss = 0.04752682\n",
      "Iteration 738, loss = 0.04742053\n",
      "Iteration 739, loss = 0.04735256\n",
      "Iteration 740, loss = 0.04720223\n",
      "Iteration 741, loss = 0.04709084\n",
      "Iteration 742, loss = 0.04702708\n",
      "Iteration 743, loss = 0.04693270\n",
      "Iteration 744, loss = 0.04677104\n",
      "Iteration 745, loss = 0.04670283\n",
      "Iteration 746, loss = 0.04680703\n",
      "Iteration 747, loss = 0.04654520\n",
      "Iteration 748, loss = 0.04646595\n",
      "Iteration 749, loss = 0.04643163\n",
      "Iteration 750, loss = 0.04625913\n",
      "Iteration 751, loss = 0.04614896\n",
      "Iteration 752, loss = 0.04608685\n",
      "Iteration 753, loss = 0.04599874\n",
      "Iteration 754, loss = 0.04587043\n",
      "Iteration 755, loss = 0.04583351\n",
      "Iteration 756, loss = 0.04576424\n",
      "Iteration 757, loss = 0.04572774\n",
      "Iteration 758, loss = 0.04551827\n",
      "Iteration 759, loss = 0.04543660\n",
      "Iteration 760, loss = 0.04541852\n",
      "Iteration 761, loss = 0.04529724\n",
      "Iteration 762, loss = 0.04514295\n",
      "Iteration 763, loss = 0.04507129\n",
      "Iteration 764, loss = 0.04499785\n",
      "Iteration 765, loss = 0.04506496\n",
      "Iteration 766, loss = 0.04486495\n",
      "Iteration 767, loss = 0.04472954\n",
      "Iteration 768, loss = 0.04471741\n",
      "Iteration 769, loss = 0.04463827\n",
      "Iteration 770, loss = 0.04460728\n",
      "Iteration 771, loss = 0.04451188\n",
      "Iteration 772, loss = 0.04463924\n",
      "Iteration 773, loss = 0.04429977\n",
      "Iteration 774, loss = 0.04416547\n",
      "Iteration 775, loss = 0.04414845\n",
      "Iteration 776, loss = 0.04414339\n",
      "Iteration 777, loss = 0.04399554\n",
      "Iteration 778, loss = 0.04397153\n",
      "Iteration 779, loss = 0.04385068\n",
      "Iteration 780, loss = 0.04377974\n",
      "Iteration 781, loss = 0.04384328\n",
      "Iteration 782, loss = 0.04363999\n",
      "Iteration 783, loss = 0.04359314\n",
      "Iteration 784, loss = 0.04352369\n",
      "Iteration 785, loss = 0.04344698\n",
      "Iteration 786, loss = 0.04338161\n",
      "Iteration 787, loss = 0.04332000\n",
      "Iteration 788, loss = 0.04325838\n",
      "Iteration 789, loss = 0.04322647\n",
      "Iteration 790, loss = 0.04319419\n",
      "Iteration 791, loss = 0.04316570\n",
      "Iteration 792, loss = 0.04311507\n",
      "Iteration 793, loss = 0.04295743\n",
      "Iteration 794, loss = 0.04288197\n",
      "Iteration 795, loss = 0.04283949\n",
      "Iteration 796, loss = 0.04273642\n",
      "Iteration 797, loss = 0.04272918\n",
      "Iteration 798, loss = 0.04271389\n",
      "Iteration 799, loss = 0.04254376\n",
      "Iteration 800, loss = 0.04248887\n",
      "Iteration 801, loss = 0.04246008\n",
      "Iteration 802, loss = 0.04234859\n",
      "Iteration 803, loss = 0.04228457\n",
      "Iteration 804, loss = 0.04224503\n",
      "Iteration 805, loss = 0.04219937\n",
      "Iteration 806, loss = 0.04219074\n",
      "Iteration 807, loss = 0.04208370\n",
      "Iteration 808, loss = 0.04200310\n",
      "Iteration 809, loss = 0.04196396\n",
      "Iteration 810, loss = 0.04188397\n",
      "Iteration 811, loss = 0.04185850\n",
      "Iteration 812, loss = 0.04174294\n",
      "Iteration 813, loss = 0.04166316\n",
      "Iteration 814, loss = 0.04165273\n",
      "Iteration 815, loss = 0.04161739\n",
      "Iteration 816, loss = 0.04157215\n",
      "Iteration 817, loss = 0.04140208\n",
      "Iteration 818, loss = 0.04138226\n",
      "Iteration 819, loss = 0.04131966\n",
      "Iteration 820, loss = 0.04130884\n",
      "Iteration 821, loss = 0.04122563\n",
      "Iteration 822, loss = 0.04109803\n",
      "Iteration 823, loss = 0.04110002\n",
      "Iteration 824, loss = 0.04100525\n",
      "Iteration 825, loss = 0.04096590\n",
      "Iteration 826, loss = 0.04094153\n",
      "Iteration 827, loss = 0.04077956\n",
      "Iteration 828, loss = 0.04080895\n",
      "Iteration 829, loss = 0.04079620\n",
      "Iteration 830, loss = 0.04063495\n",
      "Iteration 831, loss = 0.04072560\n",
      "Iteration 832, loss = 0.04048348\n",
      "Iteration 833, loss = 0.04047954\n",
      "Iteration 834, loss = 0.04039594\n",
      "Iteration 835, loss = 0.04042424\n",
      "Iteration 836, loss = 0.04023351\n",
      "Iteration 837, loss = 0.04016431\n",
      "Iteration 838, loss = 0.04007682\n",
      "Iteration 839, loss = 0.04017846\n",
      "Iteration 840, loss = 0.04004184\n",
      "Iteration 841, loss = 0.03993520\n",
      "Iteration 842, loss = 0.03995794\n",
      "Iteration 843, loss = 0.04003360\n",
      "Iteration 844, loss = 0.03976963\n",
      "Iteration 845, loss = 0.03974317\n",
      "Iteration 846, loss = 0.03964424\n",
      "Iteration 847, loss = 0.03958097\n",
      "Iteration 848, loss = 0.03951802\n",
      "Iteration 849, loss = 0.03941342\n",
      "Iteration 850, loss = 0.03942794\n",
      "Iteration 851, loss = 0.03930417\n",
      "Iteration 852, loss = 0.03930852\n",
      "Iteration 853, loss = 0.03926740\n",
      "Iteration 854, loss = 0.03909479\n",
      "Iteration 855, loss = 0.03904513\n",
      "Iteration 856, loss = 0.03897399\n",
      "Iteration 857, loss = 0.03903024\n",
      "Iteration 858, loss = 0.03901059\n",
      "Iteration 859, loss = 0.03882366\n",
      "Iteration 860, loss = 0.03876496\n",
      "Iteration 861, loss = 0.03878192\n",
      "Iteration 862, loss = 0.03871740\n",
      "Iteration 863, loss = 0.03860006\n",
      "Iteration 864, loss = 0.03854546\n",
      "Iteration 865, loss = 0.03852111\n",
      "Iteration 866, loss = 0.03844761\n",
      "Iteration 867, loss = 0.03839051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 868, loss = 0.03852813\n",
      "Iteration 869, loss = 0.03855052\n",
      "Iteration 870, loss = 0.03821240\n",
      "Iteration 871, loss = 0.03823527\n",
      "Iteration 872, loss = 0.03809318\n",
      "Iteration 873, loss = 0.03807354\n",
      "Iteration 874, loss = 0.03804641\n",
      "Iteration 875, loss = 0.03794292\n",
      "Iteration 876, loss = 0.03795935\n",
      "Iteration 877, loss = 0.03794212\n",
      "Iteration 878, loss = 0.03784247\n",
      "Iteration 879, loss = 0.03787001\n",
      "Iteration 880, loss = 0.03785284\n",
      "Iteration 881, loss = 0.03765156\n",
      "Iteration 882, loss = 0.03761817\n",
      "Iteration 883, loss = 0.03759698\n",
      "Iteration 884, loss = 0.03758110\n",
      "Iteration 885, loss = 0.03750774\n",
      "Iteration 886, loss = 0.03739467\n",
      "Iteration 887, loss = 0.03740185\n",
      "Iteration 888, loss = 0.03734942\n",
      "Iteration 889, loss = 0.03730965\n",
      "Iteration 890, loss = 0.03720931\n",
      "Iteration 891, loss = 0.03718668\n",
      "Iteration 892, loss = 0.03712350\n",
      "Iteration 893, loss = 0.03709296\n",
      "Iteration 894, loss = 0.03710877\n",
      "Iteration 895, loss = 0.03705476\n",
      "Iteration 896, loss = 0.03697809\n",
      "Iteration 897, loss = 0.03687495\n",
      "Iteration 898, loss = 0.03683578\n",
      "Iteration 899, loss = 0.03682844\n",
      "Iteration 900, loss = 0.03682865\n",
      "Iteration 901, loss = 0.03672732\n",
      "Iteration 902, loss = 0.03661084\n",
      "Iteration 903, loss = 0.03654806\n",
      "Iteration 904, loss = 0.03648952\n",
      "Iteration 905, loss = 0.03644900\n",
      "Iteration 906, loss = 0.03644156\n",
      "Iteration 907, loss = 0.03635976\n",
      "Iteration 908, loss = 0.03627851\n",
      "Iteration 909, loss = 0.03625965\n",
      "Iteration 910, loss = 0.03619463\n",
      "Iteration 911, loss = 0.03622412\n",
      "Iteration 912, loss = 0.03608784\n",
      "Iteration 913, loss = 0.03603989\n",
      "Iteration 914, loss = 0.03602272\n",
      "Iteration 915, loss = 0.03595812\n",
      "Iteration 916, loss = 0.03597523\n",
      "Iteration 917, loss = 0.03583188\n",
      "Iteration 918, loss = 0.03608574\n",
      "Iteration 919, loss = 0.03576214\n",
      "Iteration 920, loss = 0.03582451\n",
      "Iteration 921, loss = 0.03568152\n",
      "Iteration 922, loss = 0.03564285\n",
      "Iteration 923, loss = 0.03556629\n",
      "Iteration 924, loss = 0.03554408\n",
      "Iteration 925, loss = 0.03542329\n",
      "Iteration 926, loss = 0.03541408\n",
      "Iteration 927, loss = 0.03536833\n",
      "Iteration 928, loss = 0.03543339\n",
      "Iteration 929, loss = 0.03532280\n",
      "Iteration 930, loss = 0.03522165\n",
      "Iteration 931, loss = 0.03517589\n",
      "Iteration 932, loss = 0.03525067\n",
      "Iteration 933, loss = 0.03519350\n",
      "Iteration 934, loss = 0.03511863\n",
      "Iteration 935, loss = 0.03503547\n",
      "Iteration 936, loss = 0.03490580\n",
      "Iteration 937, loss = 0.03495544\n",
      "Iteration 938, loss = 0.03474702\n",
      "Iteration 939, loss = 0.03473241\n",
      "Iteration 940, loss = 0.03464281\n",
      "Iteration 941, loss = 0.03462477\n",
      "Iteration 942, loss = 0.03456310\n",
      "Iteration 943, loss = 0.03445064\n",
      "Iteration 944, loss = 0.03440862\n",
      "Iteration 945, loss = 0.03437737\n",
      "Iteration 946, loss = 0.03428717\n",
      "Iteration 947, loss = 0.03427068\n",
      "Iteration 948, loss = 0.03430715\n",
      "Iteration 949, loss = 0.03424430\n",
      "Iteration 950, loss = 0.03416433\n",
      "Iteration 951, loss = 0.03402383\n",
      "Iteration 952, loss = 0.03395169\n",
      "Iteration 953, loss = 0.03394816\n",
      "Iteration 954, loss = 0.03380697\n",
      "Iteration 955, loss = 0.03389019\n",
      "Iteration 956, loss = 0.03372334\n",
      "Iteration 957, loss = 0.03382600\n",
      "Iteration 958, loss = 0.03359235\n",
      "Iteration 959, loss = 0.03351604\n",
      "Iteration 960, loss = 0.03349355\n",
      "Iteration 961, loss = 0.03347469\n",
      "Iteration 962, loss = 0.03340017\n",
      "Iteration 963, loss = 0.03332915\n",
      "Iteration 964, loss = 0.03340064\n",
      "Iteration 965, loss = 0.03331754\n",
      "Iteration 966, loss = 0.03315254\n",
      "Iteration 967, loss = 0.03308375\n",
      "Iteration 968, loss = 0.03301111\n",
      "Iteration 969, loss = 0.03300109\n",
      "Iteration 970, loss = 0.03293902\n",
      "Iteration 971, loss = 0.03300199\n",
      "Iteration 972, loss = 0.03289597\n",
      "Iteration 973, loss = 0.03284170\n",
      "Iteration 974, loss = 0.03268309\n",
      "Iteration 975, loss = 0.03271571\n",
      "Iteration 976, loss = 0.03262000\n",
      "Iteration 977, loss = 0.03254389\n",
      "Iteration 978, loss = 0.03249653\n",
      "Iteration 979, loss = 0.03243909\n",
      "Iteration 980, loss = 0.03236795\n",
      "Iteration 981, loss = 0.03241063\n",
      "Iteration 982, loss = 0.03231614\n",
      "Iteration 983, loss = 0.03227132\n",
      "Iteration 984, loss = 0.03217280\n",
      "Iteration 985, loss = 0.03221182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(10,10), verbose=True, max_iter=1000)\n",
    "clf.fit(X=x_train,y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy 0.9875\n",
      "Testing accuracy 0.96\n"
     ]
    }
   ],
   "source": [
    "# returns accuracy\n",
    "print(\"Training accuracy\",clf.score(X=x_train,y=y_train))\n",
    "print(\"Testing accuracy\",clf.score(X=x_test,y=y_test))\n",
    "\n",
    "y_predicted = clf.predict(x_test)\n",
    "\n",
    "# Following is an alternative way to get the accuracy scores\n",
    "# print(\"Testing accuracy\",metrics.accuracy_score(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Always look at the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30  0]\n",
      " [ 3 67]]\n"
     ]
    }
   ],
   "source": [
    "m = metrics.confusion_matrix(y_test,y_predicted,labels=clf.classes_)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Better visualization of a confusion matrix\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=m,display_labels=clf.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
